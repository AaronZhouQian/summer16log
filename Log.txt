Lab Log File for the Work at Theoretical and Emprical Data Science at UCLA in Summer 2016

****************
Date: 06/21/2016
****************
The code is in the repository https://github.com/AaronZhouQian/tftutorial

Ran tfcnn.py, i.e. the tutorial code for CNN on MNIST, three times with the given
hyperparameters, with testing accuracies of 0.9925, 0.993, 0.9922.

Now we modify the hyperparameters.

#-------------------------------------------------#
Modification 1:
Added one more convolution without adding a max pooling layer.
The reason why we didn't add a max pooling layer is because after the third convolution layer,
the dimension of the images had become 7x7, and I still have to figure out the 
correct dimension of the output if I apply a 2x2 max pooling layer.

The testing accuracies were 0.9923, 0.9912, 0.9929.

The code is at https://github.com/AaronZhouQian/tftutorial/blob/master/3conv_layers.py


#------------------------------------------------#
Modification 2:
Changed the nxn dimension of the filter of the convolutional layers to be n= [3,4,5,6,7]
and ran the algorithm three times for each n=[3,4,5,6,7]

Testing accuracies:
3x3:
0.9920, 0.9908, 0.9918

4x4:
0.9921, 0.9924, 0.9923,

5x5:
0.9883, 0.9926, 0.9921

6x6:
0.9918, 0.9923, 0.9887

7x7:
0.9920, 0.9921, 0.9913

Conclusion: I do not see significant differences across different filter sizes in this range.
Bigger ranges to be tested.

The code is at https://github.com/AaronZhouQian/tftutorial/blob/master/window_hypertfcnn.py

#------------------------------------------------#
Modification 3:

Next, let's blow up the number of output channels.
We have two convolutional layers. We ran the algorithm with the sizes of the filter to be n1xn1, n2xn2 respectively,
where n1,n2=2^i, i=0,1,2,3,4,5,6,7.

The testing accuracies can be seen at https://github.com/AaronZhouQian/tftutorial/blob/master/channel_accuracies.txt



****************
Date: 06/22/2016
****************


Continuing to modify the parameters
#------------------------------------------------#
Modification 4:
Now we change the stride length (across both width and height of the inputs).

If we change the convolution stride length from 1 to 2, then after the first convolutional layer, since the original image is 28x28, 
the output of the convolutional layer would be 14x14, which would be further reduced to 7x7 by the first max pooling layer.
So let's get rid of the second conv+max_pooling layer first.

We ran the algorithm three times for each stride length l for the convolution layer for l in range(1:filter_size), 
since the stride length cannot be greater than the filter_size, which we take to be 5 in this case.

The result is at: https://github.com/AaronZhouQian/tftutorial/blob/master/conv_strides_accuracies.txt

We see that the greater the stride lengths the lower the accuracy.

The code is at: https://github.com/AaronZhouQian/tftutorial/blob/master/1conv1maxpool_pool_strides.py



I've also written the code to change the stopping criterion for the training:
We keep the validation accuracies in a queue containing three elements.
If the oldest accuracy is the maximum of all three then we stop.


****************
Date: 06/23/2016
****************

Ran the LSTM theano code on the server.

Tried to understand the code in order to replicate it in Tensorflow.



******************************
Date: 06/24/2016 - 06/26/2016
******************************

Trying to understand the code for LSTM in theano.


*****************
Date: 06/27/2016
*****************

Converting the code from LSTM from theano to tensorflow

Code can be found at https://github.com/AaronZhouQian/lstm_tf

*****************
Date: 06/28/2016
*****************

Finishing up the code conversion from LSTM from theano to tensorflow

Walked throught the code for CNN in tensorflow and lstm in Theano.

*****************
Date: 06/29/2016
*****************

Came across an interesting thing: 
currently there are still incompatibilities between numpy matrix slicing and tensorflow matrix slicing.
See the answer to the question here: 
http://stackoverflow.com/questions/37102621/why-cant-i-one-hot-encode-my-labels-with-tensorflow-bad-slice-index-none-of-t


*****************
Date: 06/30/2016
*****************
Ran across the problem that the code for LSTM in Tensorflow for the IMDB data didn't run on GPU and didn't seem to converge

*****************************
Date: 07/01/2016 - 07/02/2016
*****************************
The error message showed that the datatype INT32 is not supported for GPU.

Performed side by side comparison in order to debug, one mini-batch at a time, 
between the original code in theano and the new one in tensorflow.

On the one hand we have,
https://github.com/AaronZhouQian/theanotutorial/blob/master/tfmodified.py
on the other we have,
https://github.com/AaronZhouQian/lstm_tensorflow_imdb/blob/master/lstm_tf_imdb_modified.py

When I performed the side by side comparison, I realized that the tensorflow code was a lot slower.
The reason is because the code had to recreate all the nodes for every mini-batch. 
The reason why I did that in the beginning was because the maximum length of the sentences in each
mini-batch was unknown at compile time. 
However, I realized that I could make the code a lot faster just by compiling every nodes of the graph 
only once when the lstm object was first created. This can be achieved by performing loops over symbolic 
variables.

But one obstacle came up. In order to make the code fast, we want to create all the operations in the initialization step.
Since we want to iterate over the maximum length of the sentences in each mini-batch, which value is unknown, we would
want to use something like the scan function in theano which performs loops over symbolic variables.

However, lots of error messages came up, such as that the gradient didn't get propagated through the scan, and that 
feed_dict threw errors saying that inputs to the scan have to be from the same frame.



*****************
Date: 07/03/2016
*****************

The errors probably have to do with the currently implementation of the scan function by Tensorflow, which feels to be very shaky.
Since I couldn't fix Tensorflow's scan function implementation by myself, I decided not to use scan.
I came up with another solution: 
We only take sentences with length less than MAXLEN and take all the mask to be of that length and iterate MAXLEN for all sentences.
We would then be able to use a for loop that iterates MAXLEN times for every mini-batch of data.

The new solution worked. Initially we faced GPU resource exhaustion errors. Then I realized this was caused by the fact that
when the program multiplies the input with the embedding the operation couldn't be placed on the gpu.
So I used the tf.nn.embedding_lookup which instead, which solved the problem.

Then there was the optimization problem where it happened that neither GradientDescentOptimizer nor AdadeltaOptimizer worked,
i.e. the accuracy didn't improve at all. So I tried a third optimizer AdamOptimizer which worked wonders.

*****************************
Date: 07/04/2016 - 07/06/2016
*****************************

Kept working on the LSTM code on Tensorflow.

Walked throught the code with other people.



*****************
Date: 07/07/2016 -07/08/2016
*****************

Implemented the GRU cell.

Surprisingly both the validation and testing accuracies for the GRU cell are better than the standard LSTM cell.

Both models have similar runtime speed.

For the GRU cell with only one layer, the validation and testing accuracies reached around 87% when limited to
reviews with lengths less than 100 words.

Timining for training 20 epochs, without validation, without saving parameters:
Theano LSTM: about 56 seconds
Theano GRU: about 87 seconds

Tensorflow LSTM with mask: about 231 seconds
Tensorflow LSTM without mask: about 172 seconds
Tensorflow GRU with mask: about 299 seconds



Benchmar between theano and tf

1. Matrix Multiplications:
10000x10000 np.random.rand matrix multiplication 1 time.
Timing in seconds:
Theano:
0.44537, 0.44125, 0.44796, 0.44434, 0.44745
Tensorflow:
0.81994, 0.91022, 0.82750, 0.84350, 0.84821

2. Next, we test both packages by runing logistic regression on a small sample of the bio informatics
challenge project for 300 epochs.

The timings in seconds are:
Theano:
0.138920068741, 0.137877941132, 0.15475487709, 0.141859054565, 0.13254904747

Tensorflow:
1.77560591698, 1.73395991325, 1.71505498886, 1.73893809319, 1.77497601509

*****************
Date: 07/11/2016 - 07/14/2016
*****************
Wrote logistic regression in both tensorflow and theano for the transcription factor prediction challenge,
i.e. the ENCODE dream challenge.

Adapted the GRU code for the purpose of the ENCODE dream challenge.


*****************
Date: 07/11/2016 - 07/14/2016
*****************

Brandon and Vanessa performed the benchmark testing for logistic regression using the MNIST dataset
between Theano and Tensorflow.

They found out that Tensorflow is slower than Theano. However, what's more interesting is that
the code ran even faster on CPU than on GPU for Tensorflow. 

I think that the only explanation for this is that the data transfer to and from the GPU memory is offsetting 
the speed gain by using the GPU for matrix multiplications.

I searched online and realized that every time we call feed_dict, we're transfering data to GPU.

In order to confirm the above suspicions, I changed the matrix multiplication benchmark code.

Instead of using feed_dict at runtime, I initialized the big matrix variable during the initialize_all_variables()
function call. We had a faster timing as a result:

0.587691068649292
0.5841159820556641
0.5812911987304688
0.5897948741912842
0.569612979888916

These timings are a lot closer to the Theano timings, which has an average of about 0.44 seconds.

Then we modified the MNIST code for tensorflow, removing feed_dict.




























