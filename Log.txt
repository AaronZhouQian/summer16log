Lab Log File for the Work at Theoretical and Emprical Data Science at UCLA in Summer 2016

****************
Date: 06/21/2016
****************
The code is in the repository https://github.com/AaronZhouQian/tftutorial

Ran tfcnn.py, i.e. the tutorial code for CNN on MNIST, three times with the given
hyperparameters, with testing accuracies of 0.9925, 0.993, 0.9922.

Now we modify the hyperparameters.

#-------------------------------------------------#
Modification 1:
Added one more convolution without adding a max pooling layer.
The reason why we didn't add a max pooling layer is because after the third convolution layer,
the dimension of the images had become 7x7, and I still have to figure out the 
correct dimension of the output if I apply a 2x2 max pooling layer.

The testing accuracies were 0.9923, 0.9912, 0.9929.

The code is at https://github.com/AaronZhouQian/tftutorial/blob/master/3conv_layers.py


#------------------------------------------------#
Modification 2:
Changed the nxn dimension of the filter of the convolutional layers to be n= [3,4,5,6,7]
and ran the algorithm three times for each n=[3,4,5,6,7]

Testing accuracies:
3x3:
0.9920, 0.9908, 0.9918

4x4:
0.9921, 0.9924, 0.9923,

5x5:
0.9883, 0.9926, 0.9921

6x6:
0.9918, 0.9923, 0.9887

7x7:
0.9920, 0.9921, 0.9913

Conclusion: I do not see significant differences across different filter sizes in this range.
Bigger ranges to be tested.

The code is at https://github.com/AaronZhouQian/tftutorial/blob/master/window_hypertfcnn.py

#------------------------------------------------#
Modification 3:

Next, let's blow up the number of output channels.
We have two convolutional layers. We ran the algorithm with the sizes of the filter to be n1xn1, n2xn2 respectively,
where n1,n2=2^i, i=0,1,2,3,4,5,6,7.

The testing accuracies can be seen at https://github.com/AaronZhouQian/tftutorial/blob/master/channel_accuracies.txt



****************
Date: 06/22/2016
****************


Continuing to modify the parameters
#------------------------------------------------#
Modification 4:
Now we change the stride length (across both width and height of the inputs).

If we change the convolution stride length from 1 to 2, then after the first convolutional layer, since the original image is 28x28, 
the output of the convolutional layer would be 14x14, which would be further reduced to 7x7 by the first max pooling layer.
So let's get rid of the second conv+max_pooling layer first.

We ran the algorithm three times for each stride length l for the convolution layer for l in range(1:filter_size), 
since the stride length cannot be greater than the filter_size, which we take to be 5 in this case.

The result is at: https://github.com/AaronZhouQian/tftutorial/blob/master/conv_strides_accuracies.txt

We see that the greater the stride lengths the lower the accuracy.

The code is at: https://github.com/AaronZhouQian/tftutorial/blob/master/1conv1maxpool_pool_strides.py



I've also written the code to change the stopping criterion for the training:
We keep the validation accuracies in a queue containing three elements.
If the oldest accuracy is the maximum of all three then we stop.


****************
Date: 06/23/2016
****************

Ran the LSTM theano code on the server.

Tried to understand the code in order to replicate it in Tensorflow.



******************************
Date: 06/24/2016 - 06/26/2016
******************************

Trying to understand the code for LSTM in theano.


*****************
Date: 06/27/2016
*****************

Converting the code from LSTM from theano to tensorflow

Code can be found at https://github.com/AaronZhouQian/lstm_tf

*****************
Date: 06/28/2016
*****************

Finishing up the code conversion from LSTM from theano to tensorflow

Walked throught the code for CNN in tensorflow and lstm in Theano.

*****************
Date: 06/29/2016
*****************

Came across an interesting thing: 
currently there are still incompatibilities between numpy matrix slicing and tensorflow matrix slicing.
See the answer to the question here: 
http://stackoverflow.com/questions/37102621/why-cant-i-one-hot-encode-my-labels-with-tensorflow-bad-slice-index-none-of-t


*****************
Date: 06/30/2016
*****************
Ran across the problem that the code for LSTM in Tensorflow for the IMDB data didn't run on GPU and didn't seem to converge

*****************************
Date: 07/01/2016 - 07/02/2016
*****************************
The error message showed that the datatype INT32 is not supported for GPU.

Performed side by side comparison in order to debug, one mini-batch at a time, 
between the original code in theano and the new one in tensorflow.

On the one hand we have,
https://github.com/AaronZhouQian/theanotutorial/blob/master/tfmodified.py
on the other we have,
https://github.com/AaronZhouQian/lstm_tensorflow_imdb/blob/master/lstm_tf_imdb_modified.py


One obstacle came up. In order to make the code fast, we want to create all the operations in the initialization step.
Since we want to iterate over the maximum length of the sentences in each mini-batch, which value is unknown, we would
want to use something like the scan function in theano which performs loops over symbolic variables.

However, there is no such function available right now in Tensorflow.

































